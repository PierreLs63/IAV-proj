{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diffusion model lab \n",
    "# Part 2/2: Analysis of the latent diffusion model \n",
    "---\n",
    "- The goal of this notebook is to study the architecture of the latent diffusion model, with a diagram provided below. \n",
    "\n",
    "<img src=\"img/presentation-ldm.png\" alt=\"main diagram of a LDM model\" width=\"700\">\n",
    "\n",
    "- Since training the model takes approximately 2 hours, we will focus on running a pretrained version.\n",
    "- The conditional part of this architecture will not be studied"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The architecture of an LDM boils down to a U-Net conditioned by a time vector, whose aim is to estimate the noise added to a markov chain process at time _t_.\n",
    "\n",
    "<img src=\"img/ldm-unet.png\" alt=\"ldm-unet\" width=\"700\">\n",
    "\n",
    "\n",
    "The training and inference phases can be summarized as follows\n",
    "\n",
    "<img src=\"img/ldm-summary.png\" alt=\"ldm-summary\" width=\"600\">\n",
    "\n",
    "In this notebook, is divided in two main parts\n",
    "\n",
    "1) study of the implementation of the training phase\n",
    "2) study of the execution of the inference phase\n",
    "\n",
    "For each of the two parts, you'll essentially be analyzing code and linking it to the architectures/algorithms presented above.\n",
    "\n",
    "\n",
    "```\n",
    "Run the following cell to import the necessary libraries to work with the data and PyTorch.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import os\n",
    "import contextlib\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import monai\n",
    "from monai import transforms\n",
    "from monai.apps import MedNISTDataset\n",
    "from monai.data import Dataset, DataLoader\n",
    "from monai.data import Dataset as MonaiDataset\n",
    "from monai.networks.nets import AutoencoderKL, PatchDiscriminator, DiffusionModelUNet\n",
    "from monai.losses import PatchAdversarialLoss, PerceptualLoss\n",
    "from monai.networks.layers import Act\n",
    "from monai.inferers import LatentDiffusionInferer\n",
    "from monai.networks.schedulers import DDPMScheduler\n",
    "from monai.utils import first\n",
    "\n",
    "import numpy as np\n",
    "from torch.utils.data import random_split\n",
    "import torchvision\n",
    "from torchinfo import summary\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.amp import GradScaler, autocast\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from sklearn.manifold import TSNE\n",
    "from IPython.display import Image, display\n",
    "%matplotlib inline\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Torchvision version: {torchvision.__version__}\")\n",
    "print(f\"Numpy version: {np.__version__}\")\n",
    "print(f\"Monai version: {monai.__version__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Load and Visualize the Data\n",
    "\n",
    "Downloading may take a few moments, and you should see your progress as the data is loading.\n",
    "\n",
    "```\n",
    "Run the following cell to download and create Dataset from pytorch\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to store MedNIST dataset\n",
    "data_dir = \"data/MedNIST\"\n",
    "\n",
    "# Checks if data has already been downloaded\n",
    "download = not os.path.exists(data_dir)\n",
    "\n",
    "# Load dataset without reloading if data already exists\n",
    "train_set = MedNISTDataset(root_dir='data', section=\"training\", download=download, seed=0)\n",
    "test_set = MedNISTDataset(root_dir='data', section=\"validation\", download=download, seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Work with only the HAND class images\n",
    "\n",
    "MedNIST is composed of 8 different types of image: Abdomen from CT, Breast from MRI, Chest from CT, Pulmonary fron X-ray, Hand from X-ray and Head from CT imaging. You may also choose to change the `batch_size` if you want to load more data at a time.\n",
    "\n",
    "```\n",
    "Run the following cell to select the type of images you're going to work with.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many samples per batch to load\n",
    "batch_size = 64\n",
    "image_size = 64\n",
    "train_valid_ratio = 0.8\n",
    "minv = 0 # min intensity value of each image after rescaling\n",
    "maxv = 1 # max intensity value of each image after rescaling\n",
    "\n",
    "labels = ['AbdomenCT','BreastMRI','ChestCT','CXR','Hand','HeadCT']\n",
    "selected_label = labels[4] # ChestCt to avoid\n",
    "\n",
    "# keep only the Hand images\n",
    "train_datalist = [{\"image\": item[\"image\"], \"label\": selected_label} for item in train_set.data if item[\"class_name\"] == selected_label]\n",
    "val_datalist = [{\"image\": item[\"image\"], \"label\": selected_label} for item in test_set.data if item[\"class_name\"] == selected_label]\n",
    "\n",
    "# define the different transforms applied to the image before the generation of the dataset\n",
    "train_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.LoadImaged(keys=[\"image\"]),\n",
    "        transforms.EnsureChannelFirstd(keys=[\"image\"]),\n",
    "        transforms.ScaleIntensityRanged(keys=[\"image\"], a_min=0.0, a_max=255.0, b_min=0.0, b_max=1.0, clip=True),\n",
    "        transforms.RandAffined(\n",
    "            keys=[\"image\"],\n",
    "            rotate_range=[(-np.pi / 36, np.pi / 36), (-np.pi / 36, np.pi / 36)],\n",
    "            translate_range=[(-1, 1), (-1, 1)],\n",
    "            scale_range=[(-0.05, 0.05), (-0.05, 0.05)],\n",
    "            spatial_size=[image_size, image_size],\n",
    "            padding_mode=\"zeros\",\n",
    "            prob=0.5,\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "val_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.LoadImaged(keys=[\"image\"]),\n",
    "        transforms.EnsureChannelFirstd(keys=[\"image\"]),\n",
    "        transforms.ScaleIntensityRanged(keys=[\"image\"], a_min=0.0, a_max=255.0, b_min=0.0, b_max=1.0, clip=True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# create the train / test datasets\n",
    "train_dataset = Dataset(data=train_datalist, transform=train_transforms)\n",
    "val_dataset = Dataset(data=val_datalist, transform=val_transforms)\n",
    "\n",
    "# prepare data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4, persistent_workers=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=True, num_workers=4, persistent_workers=True)\n",
    "\n",
    "# display the size of the different datasets \n",
    "print(f\"Training dataset size: {len(train_loader.dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_loader.dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize a Batch of Training Data\n",
    "\n",
    "It's always important to check the accuracy of the data before going any further.\n",
    "\n",
    "```\n",
    "Run the cell below to display a subset of the training dataset and the size of a batch\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain one batch of training images\n",
    "dataiter_train = iter(train_loader)\n",
    "batch = next(dataiter_train)\n",
    "images = batch[\"image\"]\n",
    "images = images.numpy()\n",
    "print(f\"The image batch size is {images.shape}\")\n",
    "\n",
    "# plot the images in the batch, along with the corresponding labels\n",
    "fig = plt.figure(figsize=(25, 4))\n",
    "for idx in np.arange(20):\n",
    "    ax = fig.add_subplot(2, 10, idx+1, xticks=[], yticks=[])\n",
    "    ax.imshow(np.squeeze(images[idx]), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View an Image in More Detail\n",
    "\n",
    "```\n",
    "Run the cell below to display an image with the value of each pixel. This will enable you to check the operations performed by the transforms.Compose() function.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = np.squeeze(images[0])\n",
    "print(f\"The size of an image from the dataset is {img.shape}\")\n",
    "\n",
    "fig = plt.figure(figsize = (20,20)) \n",
    "ax = fig.add_subplot(111)\n",
    "ax.imshow(img, cmap='gray')\n",
    "width, height = img.shape\n",
    "thresh = img.max()/2.5\n",
    "for x in range(width):\n",
    "    for y in range(height):\n",
    "        val = round(img[x][y],2) if img[x][y] !=0 else 0\n",
    "        ax.annotate(f\"{val:.1f}\", xy=(y,x),\n",
    "                    horizontalalignment='center',\n",
    "                    verticalalignment='center',\n",
    "                    color='white' if img[x][y]<thresh else 'black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Define the [VAE network architecture](https://github.com/Project-MONAI/MONAI/blob/dev/monai/networks/nets/autoencoderkl.py#L564)\n",
    "\n",
    "We will create a VAE network using an pre-defined network from MONAI library. The parameters chosen are sub-optimal, but reduce the complexity of the architecture and make calculation times short enough to run during the lab.\n",
    "\n",
    "```\n",
    "Run the two cells below to define the VAE network (seen as the generator in the adversarial scheme) and visualize the main properties of its architecture.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "spatial_dims = 2\n",
    "in_channels = 1\n",
    "out_channels = 1\n",
    "channels = (128, 128, 256)\n",
    "latent_channels = 3\n",
    "num_res_blocks = 2\n",
    "norm_num_groups = channels[0]\n",
    "attention_levels = (False, False, False)\n",
    "with_encoder_nonlocal_attn = False\n",
    "with_decoder_nonlocal_attn = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# use of the with command line to avoid the automatic display of log information during the instanciation of the class model \n",
    "with contextlib.redirect_stdout(None):\n",
    "    autoencoderkl = AutoencoderKL(\n",
    "        spatial_dims=spatial_dims,\n",
    "        in_channels=in_channels,\n",
    "        out_channels=out_channels,\n",
    "        channels=channels,\n",
    "        latent_channels=latent_channels,\n",
    "        num_res_blocks=num_res_blocks,\n",
    "        norm_num_groups=norm_num_groups,\n",
    "        attention_levels=attention_levels,\n",
    "        with_encoder_nonlocal_attn=with_encoder_nonlocal_attn,\n",
    "        with_decoder_nonlocal_attn=with_decoder_nonlocal_attn,\n",
    "    )\n",
    "    autoencoderkl.to(device)\n",
    "    \n",
    "# Print the summary of the encoder network\n",
    "summary_kwargs = dict(\n",
    "    col_names=[\"input_size\", \"output_size\", \"num_params\"], depth=3, verbose=0\n",
    ")\n",
    "summary(autoencoderkl, (1, 1, image_size, image_size), device=\"cpu\", **summary_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <span style=\"color:red\">Describe the main characteristics of this network.</span>\n",
    "- <span style=\"color:red\">How many parameters do the network have in total?</span>\n",
    "- <span style=\"color:red\">What is the size of the latent space?</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below is commented. It corresponds to the code that has been executed to train the VAE. As you can see, this code is very similar to the one you studied in part 1 of this lab. You don't have to run it, but we've provided it for your information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # define perceptual loss\n",
    "# perceptual_loss = PerceptualLoss(spatial_dims=2, network_type=\"alex\")\n",
    "# perceptual_loss.to(device)\n",
    "# perceptual_weight = 0.001\n",
    "\n",
    "# # define discriminator network\n",
    "# discriminator = PatchDiscriminator(spatial_dims=2, num_layers_d=3, channels=64, in_channels=1, out_channels=1)\n",
    "# discriminator = discriminator.to(device)\n",
    "\n",
    "# # define discriminator loss\n",
    "# adv_loss = PatchAdversarialLoss(criterion=\"least_squares\")\n",
    "# adv_weight = 0.01\n",
    "\n",
    "# # define optimizers\n",
    "# optimizer_g = torch.optim.Adam(autoencoderkl.parameters(), lr=1e-4)\n",
    "# optimizer_d = torch.optim.Adam(discriminator.parameters(), lr=5e-4)\n",
    "\n",
    "# # For mixed precision training\n",
    "# scaler_g = GradScaler()\n",
    "# scaler_d = GradScaler()\n",
    "\n",
    "# kl_weight = 1e-6\n",
    "# max_epochs = 100\n",
    "# val_interval = 10\n",
    "# autoencoder_warm_up_n_epochs = 10\n",
    "\n",
    "# epoch_recon_losses = []\n",
    "# epoch_gen_losses = []\n",
    "# epoch_disc_losses = []\n",
    "# val_recon_losses = []\n",
    "# intermediary_images = []\n",
    "# num_example_images = 4\n",
    "\n",
    "# for epoch in range(max_epochs):\n",
    "#     autoencoderkl.train()\n",
    "#     discriminator.train()\n",
    "#     epoch_loss = 0\n",
    "#     gen_epoch_loss = 0\n",
    "#     disc_epoch_loss = 0\n",
    "#     progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), ncols=110)\n",
    "#     progress_bar.set_description(f\"Epoch {epoch}\")\n",
    "#     for step, batch in progress_bar:\n",
    "#         images = batch[\"image\"].to(device)\n",
    "#         optimizer_g.zero_grad(set_to_none=True)\n",
    "\n",
    "#         with autocast(\"cuda\", enabled=True):\n",
    "#             reconstruction, z_mu, z_sigma = autoencoderkl(images)\n",
    "\n",
    "#             recons_loss = F.l1_loss(reconstruction.float(), images.float())\n",
    "#             p_loss = perceptual_loss(reconstruction.float(), images.float())\n",
    "#             kl_loss = 0.5 * torch.sum(z_mu.pow(2) + z_sigma.pow(2) - torch.log(z_sigma.pow(2)) - 1, dim=[1, 2, 3])\n",
    "#             kl_loss = torch.sum(kl_loss) / kl_loss.shape[0]\n",
    "#             loss_g = recons_loss + (kl_weight * kl_loss) + (perceptual_weight * p_loss)\n",
    "\n",
    "#             if epoch > autoencoder_warm_up_n_epochs:\n",
    "#                 logits_fake = discriminator(reconstruction.contiguous().float())[-1]\n",
    "#                 generator_loss = adv_loss(logits_fake, target_is_real=True, for_discriminator=False)\n",
    "#                 loss_g += adv_weight * generator_loss\n",
    "\n",
    "#         scaler_g.scale(loss_g).backward()\n",
    "#         scaler_g.step(optimizer_g)\n",
    "#         scaler_g.update()\n",
    "\n",
    "#         if epoch > autoencoder_warm_up_n_epochs:\n",
    "#             with autocast(\"cuda\", enabled=True):\n",
    "#                 optimizer_d.zero_grad(set_to_none=True)\n",
    "\n",
    "#                 logits_fake = discriminator(reconstruction.contiguous().detach())[-1]\n",
    "#                 loss_d_fake = adv_loss(logits_fake, target_is_real=False, for_discriminator=True)\n",
    "#                 logits_real = discriminator(images.contiguous().detach())[-1]\n",
    "#                 loss_d_real = adv_loss(logits_real, target_is_real=True, for_discriminator=True)\n",
    "#                 discriminator_loss = (loss_d_fake + loss_d_real) * 0.5\n",
    "\n",
    "#                 loss_d = adv_weight * discriminator_loss\n",
    "\n",
    "#             scaler_d.scale(loss_d).backward()\n",
    "#             scaler_d.step(optimizer_d)\n",
    "#             scaler_d.update()\n",
    "\n",
    "#         epoch_loss += recons_loss.item()\n",
    "#         if epoch > autoencoder_warm_up_n_epochs:\n",
    "#             gen_epoch_loss += generator_loss.item()\n",
    "#             disc_epoch_loss += discriminator_loss.item()\n",
    "\n",
    "#         progress_bar.set_postfix(\n",
    "#             {\n",
    "#                 \"recons_loss\": epoch_loss / (step + 1),\n",
    "#                 \"gen_loss\": gen_epoch_loss / (step + 1),\n",
    "#                 \"disc_loss\": disc_epoch_loss / (step + 1),\n",
    "#             }\n",
    "#         )\n",
    "#     epoch_recon_losses.append(epoch_loss / (step + 1))\n",
    "#     epoch_gen_losses.append(gen_epoch_loss / (step + 1))\n",
    "#     epoch_disc_losses.append(disc_epoch_loss / (step + 1))\n",
    "\n",
    "#     if (epoch + 1) % val_interval == 0:\n",
    "#         autoencoderkl.eval()\n",
    "#         val_loss = 0\n",
    "#         with torch.no_grad():\n",
    "#             for _val_step, batch in enumerate(val_loader, start=1):\n",
    "#                 images = batch[\"image\"].to(device)\n",
    "\n",
    "#                 with autocast(\"cuda\", enabled=True):\n",
    "#                     reconstruction, z_mu, z_sigma = autoencoderkl(images)\n",
    "#                     # Get the first reconstruction from the first validation batch for visualisation purposes\n",
    "#                     if _val_step == 1:\n",
    "#                         intermediary_images.append(reconstruction[:num_example_images, 0])\n",
    "\n",
    "#                     recons_loss = F.l1_loss(images.float(), reconstruction.float())\n",
    "\n",
    "#                 val_loss += recons_loss.item()\n",
    "\n",
    "#         val_loss /= _val_step\n",
    "#         val_recon_losses.append(val_loss)\n",
    "#         print(f\"epoch {epoch + 1} val loss: {val_loss:.4f}\")\n",
    "# progress_bar.close()\n",
    "\n",
    "# del discriminator\n",
    "# del perceptual_loss\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# torch.save(autoencoderkl, 'autoencoderkl_for_diffusion.pth')  # Save the best model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Run the cell below to preload the network with pre-trained weights\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoderkl = torch.load(\"pretrained/autoencoderkl_for_diffusion.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Sample Test Results\n",
    "\n",
    "```\n",
    "Run the cell below to display reconstructed images taken from the test dataset in the following order: reference image (left) and reconstructed one (right), i.e. the output of the VAE network.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare next cell\n",
    "dataiter = iter(val_loader)\n",
    "\n",
    "# obtain one batch of test images\n",
    "batch_data = next(dataiter)\n",
    "\n",
    "# get sample outputs\n",
    "inputs = batch_data[\"image\"].to(device)\n",
    "recons, _, _ = autoencoderkl(inputs)\n",
    "# reconstruction images for display\n",
    "recons = recons.detach().cpu().numpy()\n",
    "inputs = inputs.detach().cpu().numpy()\n",
    "\n",
    "# Plot the image, label and prediction\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "for idx in range(3):\n",
    "    ax = fig.add_subplot(3, 2, 2*idx+1, xticks=[], yticks=[])\n",
    "    ax.imshow(np.squeeze(inputs[idx]), cmap='gray')\n",
    "    ax.set_title('Original')\n",
    "    ax = fig.add_subplot(3, 2, 2*idx+2, xticks=[], yticks=[])\n",
    "    ax.imshow(np.squeeze(recons[idx]), cmap='gray')\n",
    "    ax.set_title('Reconstructed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Define the [LDM network architecture](https://github.com/Project-MONAI/MONAI/blob/dev/monai/networks/nets/diffusion_model_unet.py)\n",
    "\n",
    "We will create a LDM network using an pre-defined network from MONAI library. The corresponding class will be used: `DiffusionModelUNet`, `DDPMScheduler` and `LatentDiffusionInferer`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define [diffusion](https://github.com/Project-MONAI/MONAI/blob/dev/monai/networks/nets/diffusion_model_unet.py) model and [scheduler](https://github.com/Project-MONAI/MONAI/blob/dev/monai/networks/schedulers/scheduler.py)\n",
    "\n",
    "In this section, we will define the diffusion model that will learn data distribution of the latent representation of the autoencoder. Together with the diffusion model, we define a beta scheduler responsible for defining the amount of noise that is added across the diffusion's model Markov chain.\n",
    "\n",
    "```\n",
    "Run the cell below to define the VAE network (seen as the generator in the adversarial scheme) and visualize the main properties of its architecture.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet = DiffusionModelUNet(\n",
    "    spatial_dims=2,\n",
    "    in_channels=3,\n",
    "    out_channels=3,\n",
    "    num_res_blocks=2,\n",
    "    channels=(128, 256, 512),\n",
    "    attention_levels=(False, True, True),\n",
    "    num_head_channels=(0, 256, 512),\n",
    ")\n",
    "\n",
    "scheduler = DDPMScheduler(num_train_timesteps=1000, schedule=\"linear_beta\", beta_start=0.0015, beta_end=0.0195)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <span style=\"color:red\">Referring to the UNet architecture diagram given in the introduction to this lab, explain what the scheduler object will be used for when training the diffusion network.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The display of the diffusion architecture properties is not straightforward and requires defining a timesteps vector. Therefore, we create a placeholder class below to enable displaying the properties of the unet object.\n",
    "\n",
    "```\n",
    "Run the cell below to analyze the architecture of the diffusion model.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WrappedModel(torch.nn.Module):\n",
    "    def __init__(self, model, timesteps):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.timesteps = timesteps\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Ajoute une dimension à `timesteps` pour correspondre à `[batch_size]`\n",
    "        timesteps = self.timesteps.expand(x.size(0))\n",
    "        return self.model(x, timesteps)\n",
    "\n",
    "# Ajoutez le wrapper avec un tenseur `timesteps` (valeur fixe, par exemple 10)\n",
    "timesteps = torch.tensor([10])  # Batch de taille 1\n",
    "wrapped_unet = WrappedModel(unet, timesteps)\n",
    "\n",
    "# Affichez le résumé\n",
    "summary_kwargs = dict(\n",
    "    col_names=[\"input_size\", \"output_size\", \"num_params\"], depth=3, verbose=0\n",
    ")\n",
    "summary(wrapped_unet, (1, 3, 16, 16), device=\"cpu\", **summary_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <span style=\"color:red\">Describe the main caracteristics of this network.</span>\n",
    "- <span style=\"color:red\">How many parameters do the network have in total?</span>\n",
    "- <span style=\"color:red\">Try to match this architectural description with the diagram provided at the beginning of this lab.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling factor\n",
    "\n",
    "As mentioned in Rombach et al. [1] Section 4.3.2 and D.1, the signal-to-noise ratio (induced by the scale of the latent space) can affect the results obtained with the LDM, if the standard deviation of the latent space distribution drifts too much from that of a Gaussian. For this reason, it is best practice to use a scaling factor to adapt this standard deviation.\n",
    "\n",
    "_Note: In case where the latent space is close to a Gaussian distribution, the scaling factor will be close to one, and the results will not differ from those obtained when it is not used._\n",
    "\n",
    "```\n",
    "Run the cell below to compute the scaling factor that will be applied to the standard deviation, ensuring the latent space approximates a Gaussian distribution.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_data = first(train_loader)\n",
    "with torch.no_grad():\n",
    "    with autocast(\"cuda\", enabled=True):\n",
    "        z = autoencoderkl.encode_stage_2_inputs(check_data[\"image\"].to(device))\n",
    "\n",
    "print(f\"Scaling factor set to {1/torch.std(z)}\")\n",
    "scale_factor = 1 / torch.std(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Inferer](https://github.com/Project-MONAI/MONAI/blob/dev/monai/inferers/inferer.py)\n",
    "\n",
    "We will now define an inferer. This type of class in MONAI takes an auto-encoder, diffusion model, and/or a scheduler, and can be used to perform a signal forward pass for a training iteration, and sample from the model. In the case of a LDM, the corresponding class corresponds to `LatentDiffusionInferer`\n",
    "\n",
    "```\n",
    "Run the cell below to define the inferer\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inferer = LatentDiffusionInferer(scheduler, scale_factor=scale_factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Training of the diffusion network\n",
    "\n",
    "\n",
    "The code below is commented. It corresponds to the code that has been executed to train the diffusion model. For information it tooks around 60 min to train it using a GPU card with 32 GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = torch.optim.Adam(unet.parameters(), lr=1e-4)\n",
    "\n",
    "# unet = unet.to(device)\n",
    "# max_epochs = 200\n",
    "# val_interval = 40\n",
    "# epoch_losses = []\n",
    "# val_losses = []\n",
    "# scaler = GradScaler()\n",
    "\n",
    "# for epoch in range(max_epochs):\n",
    "#     unet.train()\n",
    "#     autoencoderkl.eval()\n",
    "#     epoch_loss = 0\n",
    "#     progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), ncols=70)\n",
    "#     progress_bar.set_description(f\"Epoch {epoch}\")\n",
    "#     for step, batch in progress_bar:\n",
    "#         images = batch[\"image\"].to(device)\n",
    "#         optimizer.zero_grad(set_to_none=True)\n",
    "#         with autocast(\"cuda\", enabled=True):\n",
    "#             z_mu, z_sigma = autoencoderkl.encode(images)\n",
    "#             z = autoencoderkl.sampling(z_mu, z_sigma)\n",
    "#             noise = torch.randn_like(z).to(device)\n",
    "#             timesteps = torch.randint(0, inferer.scheduler.num_train_timesteps, (z.shape[0],), device=z.device).long()\n",
    "#             noise_pred = inferer(\n",
    "#                 inputs=images, diffusion_model=unet, noise=noise, timesteps=timesteps, autoencoder_model=autoencoderkl\n",
    "#             )\n",
    "#             loss = F.mse_loss(noise_pred.float(), noise.float())\n",
    "\n",
    "#         scaler.scale(loss).backward()\n",
    "#         scaler.step(optimizer)\n",
    "#         scaler.update()\n",
    "\n",
    "#         epoch_loss += loss.item()\n",
    "\n",
    "#         progress_bar.set_postfix({\"loss\": epoch_loss / (step + 1)})\n",
    "#     epoch_losses.append(epoch_loss / (step + 1))\n",
    "\n",
    "#     if (epoch + 1) % val_interval == 0:\n",
    "#         unet.eval()\n",
    "#         val_loss = 0\n",
    "#         with torch.no_grad():\n",
    "#             for _val_step, batch in enumerate(val_loader, start=1):\n",
    "#                 images = batch[\"image\"].to(device)\n",
    "\n",
    "#                 with autocast(\"cuda\", enabled=True):\n",
    "#                     z_mu, z_sigma = autoencoderkl.encode(images)\n",
    "#                     z = autoencoderkl.sampling(z_mu, z_sigma)\n",
    "\n",
    "#                     noise = torch.randn_like(z).to(device)\n",
    "#                     timesteps = torch.randint(\n",
    "#                         0, inferer.scheduler.num_train_timesteps, (z.shape[0],), device=z.device\n",
    "#                     ).long()\n",
    "#                     noise_pred = inferer(\n",
    "#                         inputs=images,\n",
    "#                         diffusion_model=unet,\n",
    "#                         noise=noise,\n",
    "#                         timesteps=timesteps,\n",
    "#                         autoencoder_model=autoencoderkl,\n",
    "#                     )\n",
    "\n",
    "#                     loss = F.mse_loss(noise_pred.float(), noise.float())\n",
    "\n",
    "#                 val_loss += loss.item()\n",
    "#         val_loss /= _val_step\n",
    "#         val_losses.append(val_loss)\n",
    "#         print(f\"Epoch {epoch} val loss: {val_loss:.4f}\")\n",
    "\n",
    "#         # Sampling image during training\n",
    "#         z = torch.randn((1, 3, 16, 16))\n",
    "#         z = z.to(device)\n",
    "#         scheduler.set_timesteps(num_inference_steps=1000)\n",
    "#         with autocast(\"cuda\", enabled=True):\n",
    "#             decoded = inferer.sample(\n",
    "#                 input_noise=z, diffusion_model=unet, scheduler=scheduler, autoencoder_model=autoencoderkl\n",
    "#             )\n",
    "\n",
    "#         plt.figure(figsize=(2, 2))\n",
    "#         plt.style.use(\"default\")\n",
    "#         plt.imshow(decoded[0, 0].detach().cpu(), vmin=0, vmax=1, cmap=\"gray\")\n",
    "#         plt.tight_layout()\n",
    "#         plt.axis(\"off\")\n",
    "#         plt.show()\n",
    "# progress_bar.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <span style=\"color:red\">Take the time to analyse this code and make the link with the different diagrams given at the introduction of this lab.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Run the cell below to preload the diffusion network and the inferer with pre-trained weights\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet = torch.load(\"pretrained/diffusion_model_unet.pth\")\n",
    "inferer = torch.load(\"pretrained/inferer_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For information, the following learning curves have been obtained at the end of the training phase.\n",
    "\n",
    "<img src=\"img/diffusion-model-loss-curves.png\" alt=\"diffusion model loss curves\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Evaluate the performance of our trained model on a test dataset\n",
    "\n",
    "### Visualize Sample Test Results\n",
    "\n",
    "```\n",
    "Run the following cell to generate a random latent matrix with values sampled from a Gaussian distribution and produce the corresponding synthetic image.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet.eval()\n",
    "scheduler.set_timesteps(num_inference_steps=1000)\n",
    "noise = torch.randn((1, 3, 16, 16))\n",
    "noise = noise.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <span style=\"color:red\">Can you comment on the chosen size of the latent matrix?</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Run the following cell to reverse propagate through the diffusion model over 1000 iterations to generate the corresponding synthetic image.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    image, intermediates = inferer.sample(\n",
    "        input_noise=noise,\n",
    "        diffusion_model=unet,\n",
    "        scheduler=scheduler,\n",
    "        save_intermediates=True,\n",
    "        intermediate_steps=100,\n",
    "        autoencoder_model=autoencoderkl,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Run the following cell to decode the latent representations over the reverse process and display the corresponding images\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode latent representation of the intermediary images\n",
    "decoded_images = []\n",
    "for image in intermediates:\n",
    "    with torch.no_grad():\n",
    "        decoded_images.append(image)\n",
    "plt.figure(figsize=(10, 12))\n",
    "chain = torch.cat(decoded_images, dim=-1)\n",
    "# plt.style.use(\"default\")\n",
    "plt.imshow(chain[0, 0].cpu(), vmin=0, vmax=1, cmap=\"gray\")\n",
    "plt.tight_layout()\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <span style=\"color:red\">Run the previous two cells and evaluate the quality of the generated synthetic images.</span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
