{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diffusion model lab \n",
    "# Part 1/2: Pretraining of the VAE part of the architecture\n",
    "---\n",
    "- The goal of this notebook is to study the VAE involved in the latent diffusion model studied in the course. This network is pre-trained and used as a fix part in the diffusion model. \n",
    "\n",
    "<img src=\"img/presentation-ldm.png\" alt=\"main diagram of a LDM model\" width=\"600\">\n",
    "\n",
    "\n",
    "- <span style=\"color:red\">Where is the VAE located in the diagram above?.</span>\n",
    "- <span style=\"color:red\">What is the purpose of using a VAE within a diffusion model?</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The VAE architecture is the same as the one you already studied in the previous lab, implemented using the `AutoencoderKL` class from MONAI. The main differences are:\n",
    "\n",
    "1) the use of an adversarial strategy to encourage the VAE to reconstruct higher-quality images;\n",
    "2) a more complex loss function, which includes a perceptual term and an adversarial term.\n",
    "\n",
    "The adversarial term is directly related to the adversarial approach, while the perceptual term is designed to improve the quality of the reconstructed images. Both aspects will be further explored in this lab. The final loss can be expressed as:\n",
    "\n",
    "<img src=\"img/final-loss.png\" alt=\"Final loss\" width=\"500\">\n",
    "\n",
    "```\n",
    "Run the following cell to import the necessary libraries to work with the data and PyTorch.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import os\n",
    "import contextlib\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import monai\n",
    "from monai.apps import MedNISTDataset\n",
    "from monai.data import Dataset, DataLoader\n",
    "from monai.data import Dataset as MonaiDataset\n",
    "from monai.transforms import Compose, LoadImage, ToTensor\n",
    "from monai.transforms import LoadImaged, EnsureChannelFirstd, ScaleIntensityRanged, Resized\n",
    "from monai.networks.nets import AutoencoderKL, PatchDiscriminator\n",
    "from monai.losses import PatchAdversarialLoss, PerceptualLoss\n",
    "from monai.networks.layers import Act\n",
    "\n",
    "import numpy as np\n",
    "from torch.utils.data import random_split\n",
    "import torchvision\n",
    "from torchinfo import summary\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from sklearn.manifold import TSNE\n",
    "from IPython.display import Image, display\n",
    "%matplotlib inline\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Torchvision version: {torchvision.__version__}\")\n",
    "print(f\"Numpy version: {np.__version__}\")\n",
    "print(f\"Monai version: {monai.__version__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Load and Visualize the Data\n",
    "\n",
    "Downloading may take a few moments, and you should see your progress as the data is loading.\n",
    "\n",
    "```\n",
    "Run the following cell to download and create Dataset from pytorch\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to store MedNIST dataset\n",
    "data_dir = \"data/MedNIST\"\n",
    "\n",
    "# Checks if data has already been downloaded\n",
    "download = not os.path.exists(data_dir)\n",
    "\n",
    "# Load dataset without reloading if data already exists\n",
    "train_set = MedNISTDataset(root_dir='data', section=\"training\", download=download, seed=0)\n",
    "test_set = MedNISTDataset(root_dir='data', section=\"validation\", download=download, seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Work with only the HAND class images\n",
    "\n",
    "MedNIST is composed of 8 different types of image: Abdomen from CT, Breast from MRI, Chest from CT, Pulmonary fron X-ray, Hand from X-ray and Head from CT imaging. You may also choose to change the `batch_size` if you want to load more data at a time.\n",
    "\n",
    "```\n",
    "Run the following cell to select the type of images you're going to work with.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many samples per batch to load\n",
    "batch_size = 64\n",
    "image_size = 48 # the native image are of size 64x64. We set image_size ot 32 to reduce the complexity of the problem and make the training process faster\n",
    "train_valid_ratio = 0.8\n",
    "minv = 0 # min intensity value of each image after rescaling\n",
    "maxv = 1 # max intensity value of each image after rescaling\n",
    "\n",
    "labels = ['AbdomenCT','BreastMRI','ChestCT','CXR','Hand','HeadCT']\n",
    "selected_label = labels[4] # ChestCt to avoid\n",
    "\n",
    "# keep only the Hand images\n",
    "train_datalist = [{\"image\": item[\"image\"], \"label\": selected_label} for item in train_set.data if item[\"class_name\"] == selected_label]\n",
    "test_datalist = [{\"image\": item[\"image\"], \"label\": selected_label} for item in test_set.data if item[\"class_name\"] == selected_label]\n",
    "\n",
    "# this part of the code remove some image cases where the hand is surrounded by a clear rectangle. \n",
    "# This trick reduces image variability and database size. This in turn reduces computation times in the lab.\n",
    "def filter_images_by_intensity(datalist, threshold):\n",
    "    filtered_datalist = []\n",
    "    for item in datalist:\n",
    "        image = LoadImage(image_only=True)(item[\"image\"])  # Load original image\n",
    "        nb_pixels = np.sum(image > 50)\n",
    "        if nb_pixels <= threshold:  # Keep the image if the number of pixels is lower than the threshold value \n",
    "            filtered_datalist.append(item)\n",
    "    return filtered_datalist\n",
    "\n",
    "# filter the data lisst\n",
    "train_datalist = filter_images_by_intensity(train_datalist, 1200)\n",
    "test_datalist = filter_images_by_intensity(test_datalist, 1200)\n",
    "\n",
    "# define the different transforms applied to the image before the generation of the dataset\n",
    "all_transforms = [\n",
    "    LoadImaged(keys=[\"image\"]),\n",
    "    EnsureChannelFirstd(keys=[\"image\"]),\n",
    "    ScaleIntensityRanged(keys=[\"image\"], a_min=0.0, a_max=255.0, b_min=minv, b_max=maxv, clip=True),\n",
    "    Resized(keys=[\"image\"], spatial_size=[image_size, image_size]),\n",
    "]\n",
    "\n",
    "# create the train / test datasets\n",
    "train_dataset = Dataset(data=train_datalist, transform=Compose(all_transforms))\n",
    "test_data = Dataset(data=test_datalist, transform=Compose(all_transforms))\n",
    "\n",
    "# split the train_data into a train (80%) and valid (20%) subdataset\n",
    "train_size = int(train_valid_ratio * len(train_dataset))    # 80% for training\n",
    "valid_size = len(train_dataset) - train_size                # 20% for validation\n",
    "train_data, valid_data = random_split(train_dataset, [train_size, valid_size])\n",
    "\n",
    "# prepare data loaders\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=4, persistent_workers=True)\n",
    "valid_loader = DataLoader(valid_data, batch_size=batch_size, shuffle=False, num_workers=4, persistent_workers=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=4, persistent_workers=True)\n",
    "\n",
    "# display the size of the different datasets \n",
    "print(f\"Training dataset size: {len(train_loader.dataset)}\")\n",
    "print(f\"Validation dataset size: {len(valid_loader.dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_loader.dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize a Batch of Training Data\n",
    "\n",
    "It's always important to check the accuracy of the data before going any further.\n",
    "\n",
    "```\n",
    "Run the cell below to display a subset of the training dataset and the size of a batch\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain one batch of training images\n",
    "dataiter_train = iter(train_loader)\n",
    "batch = next(dataiter_train)\n",
    "images = batch[\"image\"]\n",
    "images = images.numpy()\n",
    "print(f\"The image batch size is {images.shape}\")\n",
    "\n",
    "# plot the images in the batch, along with the corresponding labels\n",
    "fig = plt.figure(figsize=(25, 4))\n",
    "for idx in np.arange(20):\n",
    "    ax = fig.add_subplot(2, 10, idx+1, xticks=[], yticks=[])\n",
    "    ax.imshow(np.squeeze(images[idx]), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View an Image in More Detail\n",
    "\n",
    "```\n",
    "Run the cell below to display an image with the value of each pixel. This will enable you to check the operations performed by the transforms.Compose() function.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = np.squeeze(images[0])\n",
    "print(f\"The size of an image from the dataset is {img.shape}\")\n",
    "\n",
    "fig = plt.figure(figsize = (15,15)) \n",
    "ax = fig.add_subplot(111)\n",
    "ax.imshow(img, cmap='gray')\n",
    "width, height = img.shape\n",
    "thresh = img.max()/2.5\n",
    "for x in range(width):\n",
    "    for y in range(height):\n",
    "        val = round(img[x][y],2) if img[x][y] !=0 else 0\n",
    "        ax.annotate(f\"{val:.1f}\", xy=(y,x),\n",
    "                    horizontalalignment='center',\n",
    "                    verticalalignment='center',\n",
    "                    color='white' if img[x][y]<thresh else 'black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Define the [VAE network architecture](https://github.com/Project-MONAI/MONAI/blob/dev/monai/networks/nets/autoencoderkl.py#L564)\n",
    "\n",
    "We will create a VAE network using an pre-defined network from MONAI library. The parameters chosen are sub-optimal, but reduce the complexity of the architecture and make calculation times short enough to run during the lab.\n",
    "\n",
    "```\n",
    "Run the two cells below to define the VAE network (seen as the generator in the adversarial scheme) and visualize the main properties of its architecture.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "spatial_dims = 2\n",
    "in_channels = 1\n",
    "out_channels = 1\n",
    "channels = (16, 32, 64)\n",
    "latent_channels = 3\n",
    "num_res_blocks = 2\n",
    "norm_num_groups = channels[0]\n",
    "attention_levels = (False, False, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# use of the with command line to avoid the automatic display of log information during the instanciation of the class model \n",
    "with contextlib.redirect_stdout(None):\n",
    "    model = AutoencoderKL(\n",
    "        spatial_dims=spatial_dims,\n",
    "        in_channels=in_channels,\n",
    "        out_channels=out_channels,\n",
    "        channels=channels,\n",
    "        latent_channels=latent_channels,\n",
    "        num_res_blocks=num_res_blocks,\n",
    "        norm_num_groups=norm_num_groups,\n",
    "        attention_levels=attention_levels,\n",
    "    )\n",
    "    model.to(device)\n",
    "    \n",
    "# Print the summary of the encoder network\n",
    "summary_kwargs = dict(\n",
    "    col_names=[\"input_size\", \"output_size\", \"num_params\"], depth=3, verbose=0\n",
    ")\n",
    "summary(model, (1, 1, image_size, image_size), device=\"cpu\", **summary_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <span style=\"color:red\">Describe the main characteristics of this network.</span>\n",
    "- <span style=\"color:red\">How many parameters do the network have in total?</span>\n",
    "- <span style=\"color:red\">What is the size of the latent space?</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adversarial scheme\n",
    "\n",
    "An `adversarial scheme` is employed to encourage the VAE to reconstruct realistic images. To achieve this, a second network, called the `discriminator`, is trained alongside the VAE, which acts as the generator. The generator aims to produce realistic synthetic images, while the discriminator's goal is to determine whether its input image is synthetic (fake) or real. The corresponding diagram is given below:\n",
    "\n",
    "<img src=\"img/adversarial-scheme.png\" alt=\"Illustration of an adversarial scheme\" width=\"600\">\n",
    "\n",
    "\n",
    "```\n",
    "Run the two cells below to define the discriminator network and visualize the main properties of its architecture.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "spatial_dims = 2\n",
    "in_channels = 1\n",
    "out_channels = 1\n",
    "num_layers_d = 3\n",
    "channels = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use of the with command line to avoid the automatic display of log information during the instanciation of the class model \n",
    "with contextlib.redirect_stdout(None):    \n",
    "    discriminator = PatchDiscriminator(\n",
    "        spatial_dims=spatial_dims,\n",
    "        num_layers_d=num_layers_d,\n",
    "        channels=channels,\n",
    "        in_channels=in_channels,\n",
    "        out_channels=out_channels,\n",
    "        activation=(Act.LEAKYRELU, {\"negative_slope\": 0.2}),\n",
    "        norm=\"BATCH\",\n",
    "        bias=False,\n",
    "        padding=1,\n",
    "    )\n",
    "    discriminator.to(device)\n",
    "    \n",
    "# Print the summary of the network\n",
    "summary_kwargs = dict(\n",
    "    col_names=[\"input_size\", \"output_size\", \"num_params\"], depth=3, verbose=0\n",
    ")\n",
    "summary(discriminator, (1, 1, image_size, image_size), device=\"cpu\", **summary_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <span style=\"color:red\">Describe the main characteristics of this network.</span>\n",
    "- <span style=\"color:red\">How many parameters do the network have in total?</span>\n",
    "- <span style=\"color:red\">What is the output of this network?</span>\n",
    "- <span style=\"color:red\">What does this mean in relation to the diagram above?</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptual loss term\n",
    "\n",
    "In an auto-encoder, the `perceptual loss` is a type of loss function that compares high-level features extracted from a pretrained deep neural network (like AlexNet or VGG) rather than directly comparing pixel values. It is used to ensure that the reconstructed image not only matches the original in pixel intensity but also preserves semantic and perceptual similarity (capture by the feature maps). The corresponding implementation is given below.\n",
    "\n",
    "<img src=\"img/perceptual-loss.png\" alt=\"Illustration of the perceptual loss\" width=\"600\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Specify [Loss Functions](http://pytorch.org/docs/stable/nn.html#loss-functions) and [Optimizer](http://pytorch.org/docs/stable/optim.html)\n",
    "\n",
    "The total loss which is optmized corresponds to the sum of 4 terms:\n",
    "\n",
    "<img src=\"img/final-loss.png\" alt=\"Final loss\" width=\"500\">\n",
    "\n",
    "```\n",
    "Run the cell below to define the different terms of the total loss and the corresponding optimizers.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Specify loss and optimization functions\n",
    "learning_rate_g = 1e-4\n",
    "learning_rate_d = 5e-4\n",
    "    \n",
    "p_loss = PerceptualLoss(spatial_dims=2, network_type=\"alex\")\n",
    "p_loss.to(device)\n",
    "adv_loss = PatchAdversarialLoss(criterion=\"least_squares\")\n",
    "p_loss.to(device)\n",
    "l1_loss = nn.L1Loss()\n",
    "l1_loss.to(device)\n",
    "\n",
    "def vae_gaussian_kl_loss(mu, sigma):\n",
    "    kl_loss = 0.5 * torch.sum(mu.pow(2) + sigma.pow(2) - torch.log(sigma.pow(2)) - 1, dim=[1, 2, 3])\n",
    "    kl_loss = torch.sum(kl_loss) / kl_loss.shape[0]\n",
    "    return kl_loss\n",
    "\n",
    "def reconstruction_loss(x_reconstructed, x):\n",
    "    return l1_loss(x_reconstructed.float(), x.float())\n",
    "\n",
    "def perceptual_loss(x_reconstructed, x):\n",
    "    return p_loss(x_reconstructed.float(), x.float())\n",
    "\n",
    "def loss_function(recon_x, x, mu, sigma, kl_weight, p_weight, a_weight, logits, target_is_real, for_discriminator):    \n",
    "    recon_loss = reconstruction_loss(recon_x, x)\n",
    "    kld_loss = vae_gaussian_kl_loss(mu, sigma)\n",
    "    p_loss = perceptual_loss(recon_x, x)\n",
    "    a_loss = adv_loss(logits, target_is_real=target_is_real, for_discriminator=for_discriminator)\n",
    "    return recon_loss + kl_weight * kld_loss + p_weight * p_loss + a_weight * a_loss, recon_loss.item(), kl_weight * kld_loss.item(), p_weight * p_loss.item(), a_weight * a_loss.item()\n",
    "\n",
    "\n",
    "# Specify optimizers\n",
    "optimizer_generator = torch.optim.Adam(model.parameters(), lr=learning_rate_g)\n",
    "optimizer_discriminator = torch.optim.Adam(params=discriminator.parameters(), lr=learning_rate_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Training of the adversarial networks\n",
    "\n",
    "In an adversarial training approach, two networks are trained simultaneously: a generator and a discriminator. The generator (i.e. VAE)'s objective is to produce realistic outputs, in our case synthetic images, that resemble the target data (image) distribution. The discriminator, on the other hand, aims to distinguish between real data samples and those generated by the generator.\n",
    "\n",
    "During training, the generator tries to \"fool\" the discriminator by improving the quality of its outputs, while the discriminator learns to become better at identifying fake samples. This creates a competitive dynamic, where the generator and discriminator improve iteratively. The overall goal is to reach a point where the discriminator can no longer reliably distinguish between real and generated samples, indicating that the generator has learned to produce realistic data.\n",
    "\n",
    "```\n",
    "Run the cell below to train the adversarial networks. As you train, take a look at how the values for the training loss decrease over time. We want it to decrease while also avoiding overfitting the training data. \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of epochs to train the model\n",
    "n_epochs = 120\n",
    "kl_weight = 1e-6               # KL divergence weight loss  / default monai value: 1e-6\n",
    "perceptual_weight = 1e-3       # Perceptual weight for loss / default monai value: 1e-3\n",
    "adversarial_weight = 1e-2      # Adversarial weight for generator loss / default monai value 1e-2\n",
    "\n",
    "# Move the model to the device\n",
    "model.to(device)\n",
    "discriminator.to(device)\n",
    "\n",
    "# Lists to store loss and accuracy for each epoch\n",
    "train_generator_loss_list = []\n",
    "train_discriminator_loss_list = []\n",
    "valid_metric_list = []\n",
    "reconstruction_metric_list = []\n",
    "kld_metric_list = []\n",
    "perceptual_metric_list = []\n",
    "adversarial_metric_list = []\n",
    "\n",
    "save_best_model_from_metric = True\n",
    "best_valid_metric = float('inf')  # to track the best validation mesasure\n",
    "best_model = None  # to store the best model\n",
    "best_epoch = 0  # to track the epoch number of the best model\n",
    "\n",
    "model.train()  # prepare model for training\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # monitor training loss\n",
    "    model.train()  # ensure the model is in training mode\n",
    "    discriminator.train()\n",
    "    train_generator_loss = 0\n",
    "    train_discriminator_loss = 0\n",
    "    reconstruction_metric = 0\n",
    "    kld_metric = 0\n",
    "    perceptual_metric = 0\n",
    "    adversarial_metric = 0\n",
    "    \n",
    "    ################################\n",
    "    # train the adversarial models #\n",
    "    ################################\n",
    "    for batch_data in train_loader:\n",
    "        \n",
    "        ###################        \n",
    "        # Generator part\n",
    "        ###################        \n",
    "        \n",
    "        # Load data and target samples stored the current batch_data\n",
    "        inputs = batch_data[\"image\"].to(device)\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer_generator.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        reconstruction, z_mu, z_sigma = model(inputs)\n",
    "        # predict fake logits  \n",
    "        logits_fake = discriminator(reconstruction.contiguous().float())[-1]\n",
    "        # calculate the generator loss\n",
    "        loss_generator, reconstruction_val, kld_val, perceptual_val, adversarial_val = loss_function(\n",
    "            reconstruction, \n",
    "            inputs, \n",
    "            z_mu, \n",
    "            z_sigma, \n",
    "            kl_weight, \n",
    "            perceptual_weight,\n",
    "            adversarial_weight,\n",
    "            logits_fake,\n",
    "            target_is_real=True,\n",
    "            for_discriminator=False)\n",
    "                \n",
    "        # backpropagate\n",
    "        loss_generator.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer_generator.step()\n",
    "        # update running training generator loss\n",
    "        train_generator_loss += loss_generator.item() * inputs.size(0)\n",
    "        reconstruction_metric += reconstruction_val * inputs.size(0)\n",
    "        kld_metric += kld_val * inputs.size(0)\n",
    "        perceptual_metric += perceptual_val * inputs.size(0)\n",
    "        adversarial_metric += adversarial_val * inputs.size(0)\n",
    "        \n",
    "        ###################        \n",
    "        # Discriminator part\n",
    "        ###################        \n",
    "        \n",
    "        if adversarial_weight > 0:\n",
    "        \n",
    "            # clear the gradients of all optimized variables        \n",
    "            optimizer_discriminator.zero_grad(set_to_none=True)\n",
    "            # predict fake logits\n",
    "            logits_fake = discriminator(reconstruction.contiguous().detach())[-1]\n",
    "            # compute real logits\n",
    "            logits_real = discriminator(inputs.contiguous().detach())[-1]\n",
    "            # calculate the discriminator loss\n",
    "            loss_d_fake = adv_loss(logits_fake, target_is_real=False, for_discriminator=True)\n",
    "            loss_d_real = adv_loss(logits_real, target_is_real=True, for_discriminator=True)\n",
    "            discriminator_loss = (loss_d_fake + loss_d_real) * 0.5\n",
    "            loss_discriminator = adversarial_weight * discriminator_loss\n",
    "\n",
    "            # backpropagate\n",
    "            loss_discriminator.backward()\n",
    "            # perform a single optimization step (parameter update)        \n",
    "            optimizer_discriminator.step()\n",
    "            # update running training discriminator loss\n",
    "            train_discriminator_loss += loss_discriminator.item() * inputs.size(0)\n",
    "        \n",
    "        \n",
    "    # Calculate average training loss and accuracy over the epoch\n",
    "    train_generator_loss_list.append(train_generator_loss / len(train_loader.dataset))\n",
    "    reconstruction_metric_list.append(reconstruction_metric / len(train_loader.dataset))\n",
    "    kld_metric_list.append(kld_metric / len(train_loader.dataset))\n",
    "    perceptual_metric_list.append(perceptual_metric / len(train_loader.dataset))\n",
    "    adversarial_metric_list.append(adversarial_metric / len(train_loader.dataset))\n",
    "    \n",
    "    if adversarial_weight > 0:\n",
    "        train_discriminator_loss_list.append(train_discriminator_loss / len(train_loader.dataset))\n",
    "\n",
    "    ###################\n",
    "    # Validation step #\n",
    "    ###################    \n",
    "    model.eval()  # set model to evaluation mode\n",
    "    valid_metric = 0\n",
    "\n",
    "    with torch.no_grad():  # disable gradient calculation during validation\n",
    "        for batch_data in valid_loader:\n",
    "            # Load data and target samples stored the current batch_data\n",
    "            inputs = batch_data[\"image\"].to(device)\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            reconstruction, z_mu, z_sigma = model(inputs)\n",
    "            # calculate the loss\n",
    "            recon_val = reconstruction_loss(reconstruction.float(), inputs.float())\n",
    "            valid_metric += recon_val.item() * inputs.size(0)\n",
    "\n",
    "    # Compute average validation loss and accuracy\n",
    "    valid_metric_list.append(valid_metric / len(valid_loader.dataset))\n",
    "        \n",
    "    print(f'Epoch: {epoch+1} \\tTraining Loss: {train_generator_loss_list[-1]:.6f} \\tValidation metric: {valid_metric_list[-1]:.6f}')\n",
    "\n",
    "    if save_best_model_from_metric:\n",
    "        # Save the model if it has the best validation loss\n",
    "        if valid_metric_list[-1] < best_valid_metric:\n",
    "            best_valid_metric = valid_metric_list[-1]\n",
    "            best_model = model.state_dict()\n",
    "            best_epoch = epoch + 1  # Save the epoch number\n",
    "    else:\n",
    "        # Save the last model as best model\n",
    "        best_valid_metric = valid_metric_list[-1]\n",
    "        best_epoch = epoch + 1\n",
    "        best_model = model.state_dict()\n",
    "\n",
    "# After training, load the best model\n",
    "model.load_state_dict(best_model)\n",
    "torch.save(best_model, 'best_test_lossmodel.pth')  # Save the best model\n",
    "\n",
    "print(f\"Best model selected at epoch {best_epoch} with validation loss: {best_valid_metric:.6f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Evaluate the performance of our trained model on a test dataset\n",
    "\n",
    "```\n",
    "Run the following cell to display the evolution of the training loss and the validation metric curves to investigate the success of the learning process\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss curves\n",
    "plt.figure(figsize=(16, 6))\n",
    "\n",
    "# Plotting global loss\n",
    "plt.subplot(1, 5, 1)\n",
    "plt.plot(train_generator_loss_list, color=\"C0\", linewidth=2.0, label='Training Loss')\n",
    "plt.title('Training loss curve')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plotting Reconstruction loss\n",
    "plt.subplot(1, 5, 2)\n",
    "plt.plot(reconstruction_metric_list, color=\"C0\", linewidth=2.0, label='Reconstruction metric')\n",
    "plt.title('Reconstruction metric')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss (%)')\n",
    "plt.legend()\n",
    "\n",
    "# Plotting KL loss\n",
    "plt.subplot(1, 5, 3)\n",
    "plt.plot(kld_metric_list, color=\"C0\", linewidth=2.0, label='KL divergence metric')\n",
    "plt.title('KL divergence metric')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss (%)')\n",
    "plt.legend()\n",
    "\n",
    "# Plotting Perceptual loss\n",
    "plt.subplot(1, 5, 4)\n",
    "plt.plot(perceptual_metric_list, color=\"C0\", linewidth=2.0, label='Perceputal metric')\n",
    "plt.title('Perceptual metric')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss (%)')\n",
    "plt.legend()\n",
    "\n",
    "# Plotting Perceptual loss\n",
    "plt.subplot(1, 5, 5)\n",
    "plt.plot(adversarial_metric_list, color=\"C0\", linewidth=2.0, label='Adversarial metric')\n",
    "plt.title('Adversarial metric')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss (%)')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <span style=\"color:red\">Analyse the evolution of the different curves and compare the weight of each in relation to the total loss.</span>\n",
    "- <span style=\"color:red\">What conclusions can be drawn?</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the generator and discriminator losses\n",
    "\n",
    "```\n",
    "Run the cell below to investigate the evolution of the loss function of the generator and the discriminator to analyze the influence of each term\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if adversarial_weight > 0:\n",
    "\n",
    "    # Express the adversarial terms without the weight coefficient\n",
    "    generator_list = [x / adversarial_weight for x in adversarial_metric_list]\n",
    "    discriminator_list = [x / adversarial_weight for x in train_discriminator_loss_list]\n",
    "\n",
    "    plt.title(\"Adversarial Training Curves\", fontsize=20)\n",
    "    plt.plot(np.linspace(1, n_epochs, n_epochs), generator_list, color=\"C0\", linewidth=2.0, label=\"Generator\")\n",
    "    plt.plot(np.linspace(1, n_epochs, n_epochs), discriminator_list, color=\"C1\", linewidth=2.0, label=\"Discriminator\")\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.xlabel(\"Epochs\", fontsize=16)\n",
    "    plt.ylabel(\"Loss\", fontsize=16)\n",
    "    plt.legend(prop={\"size\": 14})\n",
    "    \n",
    "    # Save the figure as a PNG file\n",
    "    plt.savefig(\"img/adversarial_training_curves.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This figure has been automotically saved in the folder img with the name \"adversarial-training-curves.png\". You will use chat-gpt to help you to analysis these curves.\n",
    "\n",
    "```\n",
    "Upload the image on chat-pgt and type the command prompt bellow\n",
    "```\n",
    "\n",
    "\"I trained an adversarial approach to train a VAE from the AutoencoderKL class from Monai. I obtained the folloing curves for the generator and discriminator losses, can you help me to analysis them ? What conclusions can be drawn from this figure ?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <span style=\"color:red\">Based on chat-gpt feedback, what is your own analysis of these curves?</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the overall performance on the test dataset\n",
    "\n",
    "We then test our best model on previously unseen **test data** and evaluate it's performance. Testing on unseen data is a good way to check that our model generalizes well.\n",
    "\n",
    "`model.eval()` will set all the layers in your model to evaluation mode. This affects layers like dropout layers that turn \"off\" nodes during training with some probability, but should allow every node to be \"on\" for evaluation!\n",
    "\n",
    "```\n",
    "Run the following cell to compute the scores of the trained network on the test dataset\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize lists to monitor test loss and accuracy\n",
    "test_metric = 0.0\n",
    "\n",
    "model.eval() # prep model for *evaluation*\n",
    "\n",
    "with torch.no_grad():  # disable gradient calculation during validation\n",
    "    for batch_data in test_loader:\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        inputs = batch_data[\"image\"].to(device)\n",
    "        reconstruction, z_mu, z_sigma = model(inputs)\n",
    "        # calculate the loss\n",
    "        recon_val = reconstruction_loss(reconstruction.float(), inputs.float())\n",
    "        test_metric += recon_val.item() * inputs.size(0)    \n",
    "    \n",
    "# calculate and print avg test loss\n",
    "test_metric = test_metric / len(test_loader.dataset)\n",
    "print('Test reconstruction metric: {:.6f}\\n'.format(test_metric))\n",
    "\n",
    "# Prepare next cell\n",
    "dataiter = iter(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <span style=\"color:red\">Compare the scores obtained on the validation data and the test data. Conclusions?</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Sample Test Results\n",
    "\n",
    "```\n",
    "Run the cell below to display reconstructed images taken from the test dataset in the following order: reference image (left) and reconstructed one (right), i.e. the output of the VAE network.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain one batch of test images\n",
    "batch_data = next(dataiter)\n",
    "batch_data = next(dataiter)\n",
    "\n",
    "# get sample outputs\n",
    "inputs = batch_data[\"image\"].to(device)\n",
    "recons, _, _ = model(inputs)\n",
    "# reconstruction images for display\n",
    "recons = recons.detach().cpu().numpy()\n",
    "inputs = inputs.detach().cpu().numpy()\n",
    "\n",
    "# Plot the image, label and prediction\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "for idx in range(3):\n",
    "    ax = fig.add_subplot(3, 2, 2*idx+1, xticks=[], yticks=[])\n",
    "    ax.imshow(np.squeeze(inputs[idx]), cmap='gray')\n",
    "    ax.set_title('Original')\n",
    "    ax = fig.add_subplot(3, 2, 2*idx+2, xticks=[], yticks=[])\n",
    "    ax.imshow(np.squeeze(recons[idx]), cmap='gray')\n",
    "    ax.set_title('Reconstructed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <span style=\"color:red\">Can you comment on the difference in reconstruction quality compared to the results obtained with the previous Jupyter notebooks ?</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize latent space thanks to t-SNE\n",
    "\n",
    "It can be useful to visualize the latent space to study its structure. Since the dimensions of the latent space are not in 2D, we use a more sophisticated method, called t-SNE, to perform this visualization.\n",
    "\n",
    "```\n",
    "Run the 2 cells below to compute and display the latent space\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downsampling_ratio = 1\n",
    "counter = 0  # counter initialisation\n",
    "\n",
    "model.eval() # prep model for *evaluation*\n",
    "z_mu_accumulated = []\n",
    "\n",
    "with torch.no_grad():  # Deactivate the gradient computations\n",
    "    for batch_data in test_loader:\n",
    "        counter += 1\n",
    "        if counter % downsampling_ratio == 0:\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            inputs = batch_data[\"image\"].to(device)\n",
    "            z_mu, _ = model.encode(inputs)\n",
    "            z_mu_accumulated.append(z_mu.cpu().numpy())\n",
    "      \n",
    "z_mu_accumulated = np.concatenate(z_mu_accumulated, axis=0)\n",
    "\n",
    "z_mu_flattened = z_mu_accumulated.reshape(z_mu_accumulated.shape[0], -1)\n",
    "print(f\"Size of the latent matrix passed to the t-SNE method (Nb Sample, vector dimensionality) = {z_mu_flattened.shape}\")\n",
    "\n",
    "# Apply t-SNE to reduce the dimensionality to 2 and allows a visualization of the latent space\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "z_mu_tsne = tsne.fit_transform(z_mu_flattened)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_grid = True\n",
    "fig, ax = plt.subplots(figsize=(8, 8) if not show_grid else (4, 4))\n",
    "\n",
    "# create a scatter plot of the embeddings, colored by the labels\n",
    "scatter = ax.scatter(\n",
    "    x=z_mu_tsne[:, 0], \n",
    "    y=z_mu_tsne[:, 1], \n",
    "    cmap=\"tab10\", \n",
    "    s=10, \n",
    "    alpha=0.9,\n",
    "    zorder=2,\n",
    "    )\n",
    "\n",
    "# remove the top and right spines from the plot\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "ax.spines[\"top\"].set_visible(False)\n",
    "# if show is True, display the plot\n",
    "if show_grid:\n",
    "    # add a grid to the plot\n",
    "    ax.grid(True, color=\"lightgray\", alpha=1.0, zorder=0)\n",
    "    # plt.show()\n",
    "# otherwise, save the plot to a file and close the figure\n",
    "else:\n",
    "    plt.savefig(\"latent_space.png\", bbox_inches=\"tight\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <span style=\"color:red\">What does the size of the latent matrix passed to the t-SNE method correspond to ?</span>\n",
    "- <span style=\"color:red\">What is the size of the matrix generated by the t-SNE method ?</span>\n",
    "- <span style=\"color:red\">Analyze and interpret the latent space obtained ?</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot random images\n",
    "\n",
    "Now it is time to test the capacity of generating new synthetic digits. This is done in two steps:\n",
    "1) samples directly from the latent space;\n",
    "2) use the decoder to project the sample into the image space to generate the synthetic image.\n",
    "\n",
    "```\n",
    "Run the cell below to compute and display the latent space from a grid of points sampled directed from the latent space\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_generation = False\n",
    "\n",
    "if random_generation:\n",
    "    z_mu = torch.randn(1, 4, 8, 8).to(device)\n",
    "else:\n",
    "    inputs = batch_data[\"image\"].to(device)\n",
    "    print(inputs.shape)\n",
    "    input = inputs[2]\n",
    "    input = input.unsqueeze(0)\n",
    "    print(f\"The input image is of size {input.shape}\")\n",
    "    z_mu, _ = model.encode(input)\n",
    "\n",
    "print(f\"The latent sample is of size {z_mu.shape}\")\n",
    "\n",
    "# Decode latent sample\n",
    "reconstruction = model.decode(z_mu)\n",
    "print(f\"The reconstrudted image is of size {reconstruction.shape}\")\n",
    "\n",
    "img = reconstruction.squeeze().detach().cpu().numpy()\n",
    "img = np.squeeze(img)\n",
    "\n",
    "fig = plt.figure(figsize = (4,4)) \n",
    "ax = fig.add_subplot(111, xticks=[], yticks=[])\n",
    "ax.imshow(img, cmap='gray')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's create a cool animation ;)\n",
    "\n",
    "```\n",
    "Run the three cells below to generate an animation that allows you to study the properties of latent space. \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly select two points in the latent space\n",
    "inputs = batch_data[\"image\"].to(device)\n",
    "input = inputs[2]\n",
    "input = input.unsqueeze(0)\n",
    "latent_1, _ = model.encode(input)\n",
    "input = inputs[4]\n",
    "input = input.unsqueeze(0)\n",
    "latent_2, _ = model.encode(input)\n",
    "\n",
    "synthetic_1 = model.decode(latent_1)\n",
    "synthetic_2 = model.decode(latent_2)\n",
    "\n",
    "img_1 = synthetic_1.squeeze().detach().cpu().numpy()\n",
    "img_1 = np.squeeze(img_1)\n",
    "img_2 = synthetic_2.squeeze().detach().cpu().numpy()\n",
    "img_2 = np.squeeze(img_2)\n",
    "\n",
    "fig = plt.figure(figsize = (5,5)) \n",
    "ax = fig.add_subplot(1,2,1, xticks=[], yticks=[])\n",
    "ax.imshow(img_1, cmap='gray')\n",
    "ax = fig.add_subplot(1,2,2, xticks=[], yticks=[])\n",
    "ax.imshow(img_2, cmap='gray')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_images(model, latent_1, latent_2, steps=10):\n",
    "    # Interpolate between point1 and point2 in the latent space\n",
    "    \n",
    "    latent_1.to(device)\n",
    "    latent_2.to(device)\n",
    "    t_values = torch.linspace(0, 1, steps).to(device)    \n",
    "    latent_tmp = [torch.lerp(latent_1, latent_2, t).to(device) for t in t_values]\n",
    "    latent_interp = torch.stack([latent.squeeze(0) for latent in latent_tmp], dim=0)\n",
    "    synthetic_interp = model.decode(latent_interp)\n",
    "    \n",
    "    # Return images as a list after detaching and converting to numpy\n",
    "    return [img.squeeze().detach().cpu().numpy() for img in synthetic_interp]\n",
    "\n",
    "def save_animation_as_gif(images, filename=\"animation.gif\", interval=200):\n",
    "    fig, ax = plt.subplots(figsize = (2,2))\n",
    "    img_display = ax.imshow(images[0], cmap=\"gray\", vmin=0, vmax=1)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "    def update(frame):\n",
    "        img_display.set_data(images[frame])\n",
    "        return [img_display]\n",
    "\n",
    "    ani = FuncAnimation(fig, update, frames=len(images), interval=interval, blit=True)\n",
    "    ani.save(filename, writer=\"pillow\", fps=1000//interval)\n",
    "    plt.close(fig) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpolate between the two points and decode to generate images\n",
    "images = interpolate_images(model, latent_1, latent_2, steps=64)\n",
    "\n",
    "# Animate the interpolated images\n",
    "filename = \"mnist_interpolation.gif\"\n",
    "save_animation_as_gif(images, filename=filename, interval=100)\n",
    "\n",
    "# Affiche le GIF dans Jupyter\n",
    "display(Image(filename=filename))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Additional questions\n",
    "\n",
    "Now that you have mastered this code, try experimenting with the properties of the two networks to study their influence on the quality of the results\n",
    "\n",
    "- <span style=\"color:red\">Evaluate the influence of the kl_weight, perceptual_weight and adversarial_weight values, setting some of them to 0.</span>\n",
    "- <span style=\"color:red\">Evaluate the influence of the two network architectures. Try for instance to modify the number of layers (through channels), the type of layer (through num_res_blocks) or the latent dimension (through latent_channels). Be careful with the number of associated parameters so that the model to train is not too large.</span>\n",
    "- <span style=\"color:red\">Try to optimize the different networks to get the best results.</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
